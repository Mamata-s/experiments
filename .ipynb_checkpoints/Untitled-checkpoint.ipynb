{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e64591f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 256)\n",
      "torch.Size([1, 1, 360, 256]) tensor(1.) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from torchvision.transforms import functional as F\n",
    "import numpy as np\n",
    "\n",
    "lr_2_i = cv2.imread(\"images/lr_f1_160_2_z_47.png\").astype(np.float32) / 255.\n",
    "lr_4_i = cv2.imread(\"images/lr_f1_160_4_z_47.png\").astype(np.float32) / 255.\n",
    "lr_6_i = cv2.imread(\"images/lr_f1_160_6_z_47.png\").astype(np.float32) / 255.\n",
    "lr_8_i = cv2.imread(\"images/lr_f1_160_8_z_47.png\").astype(np.float32) / 255.\n",
    "hr_i = cv2.imread(\"images/hr_f1_160_z_47.png\").astype(np.float32) / 255.\n",
    "\n",
    "\n",
    "\n",
    "lr_2_i = cv2.cvtColor(lr_2_i, cv2.COLOR_BGR2GRAY)\n",
    "lr_4_i = cv2.cvtColor(lr_4_i,cv2.COLOR_BGR2GRAY)\n",
    "lr_6_i = cv2.cvtColor(lr_6_i,cv2.COLOR_BGR2GRAY)\n",
    "lr_8_i = cv2.cvtColor(lr_8_i,cv2.COLOR_BGR2GRAY)\n",
    "hr_i = cv2.cvtColor(hr_i,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "\n",
    "print(hr_i.shape)\n",
    "\n",
    "lr_2 = F.to_tensor(lr_2_i).unsqueeze(0)\n",
    "lr_4 = F.to_tensor(lr_4_i).unsqueeze(0)\n",
    "lr_6 = F.to_tensor(lr_6_i).unsqueeze(0)\n",
    "lr_8 = F.to_tensor(lr_8_i).unsqueeze(0)\n",
    "hr = F.to_tensor(hr_i).unsqueeze(0)\n",
    "\n",
    "\n",
    "print(hr.shape,hr.max(),hr.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd75d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils\n",
    "def convert_image(in_tensor):\n",
    "    in_tensor = in_tensor.squeeze(0)\n",
    "    # gray_scale = in_tensor[0].squeeze(0)\n",
    "    gray_scale = torch.sum(in_tensor,0)\n",
    "    gray_scale = gray_scale / in_tensor.shape[0]\n",
    "    # print(gray_scale.shape)\n",
    "    # print('gray scale range', gray_scale.min(),gray_scale.max())\n",
    "    gray_scale = gray_scale.detach().to('cpu').numpy()\n",
    "    gray_scale = gray_scale*255.\n",
    "    gray_scale =gray_scale.astype(np.uint8)\n",
    "    return gray_scale\n",
    "\n",
    "'''reduce learning rate of optimizer by half on every  150 and 225 epochs'''\n",
    "def adjust_learning_rate(optimizer, epoch,lr):\n",
    "    if epoch % 150 == 0 or epoch % 250==0:\n",
    "        lr = lr * 0.5\n",
    "    # log to TensorBoard\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2c2f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "\n",
    "import functools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def make_layer(block, n_layers):\n",
    "    layers = []\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(block())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ResidualDenseBlock_5C(nn.Module):\n",
    "    def __init__(self, nf=64, gc=32, bias=True):\n",
    "        super(ResidualDenseBlock_5C, self).__init__()\n",
    "        # gc: growth channel, i.e. intermediate channels\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1, bias=bias)\n",
    "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1, bias=bias)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "        # initialization\n",
    "        # mutil.initialize_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat((x, x1), 1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(torch.cat((x, x1, x2, x3, x4), 1))\n",
    "        return x5 * 0.2 + x\n",
    "\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    '''Residual in Residual Dense Block'''\n",
    "\n",
    "    def __init__(self, nf, gc=32):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.RDB1 = ResidualDenseBlock_5C(nf, gc)\n",
    "        self.RDB2 = ResidualDenseBlock_5C(nf, gc)\n",
    "        self.RDB3 = ResidualDenseBlock_5C(nf, gc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.RDB1(x)\n",
    "        out = self.RDB2(out)\n",
    "        out = self.RDB3(out)\n",
    "        return out * 0.2 + x\n",
    "\n",
    "\n",
    "class RRDBNet(nn.Module):\n",
    "    def __init__(self, in_nc, out_nc, nf, nb, gc=32):\n",
    "        super(RRDBNet, self).__init__()\n",
    "        RRDB_block_f = functools.partial(RRDB, nf=nf, gc=gc)\n",
    "\n",
    "        self.conv_first = nn.Conv2d(in_nc, nf, 3, 1, 1, bias=True)\n",
    "        self.RRDB_trunk = make_layer(RRDB_block_f, nb)\n",
    "        self.trunk_conv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        #### upsampling\n",
    "        self.upconv1 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.upconv2 = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.HRconv = nn.Conv2d(nf, nf, 3, 1, 1, bias=True)\n",
    "        self.conv_last = nn.Conv2d(nf, out_nc, 3, 1, 1, bias=True)\n",
    "\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        fea1 = self.conv_first(x)\n",
    "        trunk = self.trunk_conv(self.RRDB_trunk(fea1))\n",
    "        fea2 = fea1 + trunk\n",
    "\n",
    "        fea3 = self.lrelu(self.upconv1(F.interpolate(fea2, scale_factor=1, mode='nearest')))\n",
    "        fea4 = self.lrelu(self.upconv2(F.interpolate(fea3, scale_factor=1, mode='nearest')))\n",
    "        out = self.conv_last(self.lrelu(self.HRconv(fea4)))\n",
    "\n",
    "        return {'fea1':fea1, 'trun':trunk,'fea2':fea2,'fea3':fea3,'out':out}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b08be7c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:thplawvo) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">RRDB_MSE_LR0.01_NEP255_LOSSWT1._WAND</strong>: <a href=\"https://wandb.ai/super_resolution/debugging-nn/runs/thplawvo\" target=\"_blank\">https://wandb.ai/super_resolution/debugging-nn/runs/thplawvo</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220617_171351-thplawvo/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:thplawvo). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.17"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/cidar/Desktop/experiments/network_architecture/wandb/run-20220617_171430-3tamv3sa</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/super_resolution/debugging-nn/runs/3tamv3sa\" target=\"_blank\">RRDB_MSE_LR0.01_NEP255_LOSSWT1._WAND</a></strong> to <a href=\"https://wandb.ai/super_resolution/debugging-nn\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_848292/3594112473.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fea1 shape'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fea1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "#training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(project='debugging-nn',name=f\"RRDB_MSE_LR0.01_NEP255_LOSSWT1._WAND\")\n",
    "\n",
    "torch.manual_seed(123) \n",
    "\n",
    "\n",
    "net = RRDBNet(1, 1, 64, 23, gc=32)\n",
    "# loss_fn = nn.L1Loss()\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "wandb.watch(net,log=\"all\",log_freq=1)\n",
    "\n",
    "label = hr\n",
    "input_model = lr_2\n",
    "lr = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "# optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "epochs = 255\n",
    "n_freq = 50\n",
    "loss_wt = 1.\n",
    "\n",
    "for i in range(epochs):\n",
    "    lr = adjust_learning_rate(optimizer,i,lr)\n",
    "    result = net(input_model)\n",
    "    output = result['out']\n",
    "    if i==0:\n",
    "        print('fea1 shape',result['fea1'].shape)\n",
    "        print('trun shape',result['trun'].shape)\n",
    "        print('fea2',result['fea2'].shape)\n",
    "        print('fea3',result['fea3'].shape)\n",
    "\n",
    "    fea1 = convert_image(result['fea1'])\n",
    "    trun = convert_image(result['trun'])\n",
    "    fea2 = convert_image(result['fea2'])\n",
    "    fea3 = convert_image(result['fea3'])\n",
    "    output = convert_image(result['out'])\n",
    "\n",
    "    fea1_image = wandb.Image(fea1, caption=\"feature1\")      \n",
    "    wandb.log({\"fea1\": fea1_image})\n",
    "    \n",
    "    fea2_image = wandb.Image(fea2, caption=\"feature2\")      \n",
    "    wandb.log({\"fea2\": fea2_image})\n",
    "    \n",
    "    fea3_image = wandb.Image(fea3, caption=\"feature1\")      \n",
    "    wandb.log({\"fea1\": fea3_image})\n",
    "\n",
    "    trun_image = wandb.Image(trun, caption=\"trun feature map\")      \n",
    "    wandb.log({\"trun_image\": trun_image})\n",
    "\n",
    "    output = wandb.Image(output, caption=\"output\")      \n",
    "    wandb.log({\"output\": output})\n",
    "\n",
    "\n",
    "    loss = loss_fn(label,output)*loss_wt\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    wandb.log({\"loss\":loss})\n",
    "    wandb.log({\"learning-rate\":lr})\n",
    "\n",
    "    if i % n_freq == 0:\n",
    "        print(f'range output: {output.min()},{output.max()}')\n",
    "        print('loss:', loss.item())\n",
    "    # for p in net.parameters():\n",
    "    #   print(p.grad*lr)# or whatever other operation\n",
    "wandb.unwatch(net)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2bb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d857ded",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
